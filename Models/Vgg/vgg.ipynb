{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03417fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import math\n",
    "from IPython.display import display\n",
    "def datasetImageFolder(start_path,split=None):\n",
    "    filePath = getFilePaths(start_path,[\"jpg\",\"png\",\"jpeg\"])\n",
    "    splits =  list(set(map(lambda x: x[x.find('\\\\',len(start_path)-1 )+1:x.find('\\\\',x.find('\\\\',len(start_path)-1 )+1) ] , filePath)))\n",
    "    \n",
    "    allSplits = []\n",
    "    for i in range(len(splits)):\n",
    "        rs = list(map(lambda x : x if splits[i] in x else None, filePath))\n",
    "        result = [x for x in rs if x]\n",
    "        allSplits.append(result)\n",
    "    \n",
    "    datasets = dict()\n",
    "    for i in range(len(allSplits)):\n",
    "        images  = tf.data.Dataset.from_tensor_slices(allSplits[i])\n",
    "        datasets[splits[i]] = images\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "def getFilePaths(start_path,extensions= None):\n",
    "    paths = []\n",
    "    if(extensions is not None):\n",
    "        for extension in extensions:\n",
    "            paths.extend(list(Path(start_path).rglob(\"*.\" + extension)))\n",
    "    \n",
    "    else:\n",
    "        path.extend(list(Path(start_path).rglob()))\n",
    "    \n",
    "    return [str(x) for x in paths if x.is_file()]\n",
    "    \n",
    "    \n",
    "def getLabelFromFilePathTF(file_path):\n",
    "    splits = tf.strings.split(file_path,'\\\\')\n",
    "    label = splits[len(splits)-2]\n",
    "    return label\n",
    "\n",
    "def getLabelFromFilePath(file_path):\n",
    "    lastIndex = file_path.rfind('\\\\')\n",
    "    firstIndex = file_path.rfind('\\\\',0,lastIndex)+1\n",
    "    label =file_path[firstIndex:lastIndex]\n",
    "    \n",
    "    return label\n",
    "\n",
    "def getAllLabels(start_path,generateUndefined=False):\n",
    "    paths = getFilePaths(start_path,[\"jpg\",\"png\",\"jpeg\"])\n",
    "    distinctLabels = []\n",
    "    if (generateUndefined):\n",
    "        distinctLabels.append('Undefined')\n",
    "    \n",
    "    distinctLabels.extend(list(set(map(lambda x: getLabelFromFilePath(x) , paths))))\n",
    "        \n",
    "    return distinctLabels\n",
    "\n",
    "def generateOneHotEncodeDict(labels):\n",
    "    indices = [x for x in range(len(labels))]\n",
    "    one_hots = tf.one_hot(indices,len(labels),dtype=tf.uint8)\n",
    "    \n",
    "    one_hot_dict = dict()\n",
    "    for i in range(len(one_hots)):\n",
    "        one_hot_dict[labels[i]] = one_hots[i]\n",
    "        \n",
    "    return one_hot_dict\n",
    "    \n",
    "def generateBinaryEncoding(labels):\n",
    "    indices = [x for x in range(len(labels))]\n",
    "    n_bits = int(math.log(len(labels),2)) + 1 \n",
    "    binaryEncode = []\n",
    "    for index in indices:\n",
    "        encoding = []\n",
    "        for i in range(n_bits):\n",
    "            encoding.append( (index >> i) & 1)\n",
    "        encoding.reverse()\n",
    "        binaryEncode.append(encoding)\n",
    "    \n",
    "    \n",
    "    binaryEncodeTensors = [tf.convert_to_tensor(x,dtype=tf.uint8) for x in binaryEncode]\n",
    "    \n",
    "    binary_encode_dict = dict()\n",
    "    for i in range(len(labels)):\n",
    "        binary_encode_dict[labels[i]] = binaryEncodeTensors[i]\n",
    "        \n",
    "    return binary_encode_dict\n",
    "\n",
    "def loadBestSavedModel(start_path):        \n",
    "    models = getFilePaths(start_path,['h5'])\n",
    "    \n",
    "    bestModel = -1\n",
    "    modelToLoad = \"\"\n",
    "    \n",
    "    for modelPath in models:\n",
    "        firstIndex = modelPath.find(\".\",modelPath.find(\".\")+1)+1\n",
    "        \n",
    "        \n",
    "        modelNumber = modelPath[firstIndex:modelPath.rfind(\".\")]\n",
    "        \n",
    "        if(float(modelNumber) > bestModel ):\n",
    "            bestModel = float(modelNumber)\n",
    "            modelToLoad = modelPath\n",
    "    \n",
    "    print(modelToLoad)\n",
    "    #if len(modelToLoad) > 0:\n",
    "       #return keras.models.load_model(modelToLoad)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return modelToLoad\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6154e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D,Input,Dense,Flatten,LeakyReLU,MaxPooling2D,Dropout,Softmax,ReLU\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow_datasets as tfds\n",
    "from PIL import Image\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a02e6878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg(filters=64,n_Class=1000,shape=(224,224,3)):\n",
    "    \n",
    "    def convBlock(input_tensor,filters=filters):\n",
    "        x = Conv2D(filters,kernel_size=3,strides=1,padding='same',kernel_initializer=tf.keras.initializers.HeUniform(),kernel_regularizer=l2(0.0001))(input_tensor)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        return x\n",
    "        \n",
    "         \n",
    "    input_tensor = Input(shape=shape)\n",
    "    \n",
    "    x = convBlock(input_tensor,filters)\n",
    "    x = convBlock(x,filters)\n",
    "    x = MaxPooling2D(pool_size=(2,2),strides=2)(x)\n",
    "    \n",
    "    x = convBlock(x,filters*2)\n",
    "    x = convBlock(x,filters*2)\n",
    "    x = MaxPooling2D(pool_size=(2,2),strides=2)(x)\n",
    "    \n",
    "    x = convBlock(x,filters*4)\n",
    "    x = convBlock(x,filters*4)\n",
    "    x = convBlock(x,filters*4)\n",
    "    x = convBlock(x,filters*4)\n",
    "    x = MaxPooling2D(pool_size=(2,2),strides=2)(x)\n",
    "    \n",
    "    x = convBlock(x,filters*8)\n",
    "    x = convBlock(x,filters*8)\n",
    "    x = convBlock(x,filters*8)\n",
    "    x = convBlock(x,filters*8)\n",
    "    x = MaxPooling2D(pool_size=(2,2),strides=2)(x)\n",
    "    \n",
    "    x = convBlock(x,filters*8)\n",
    "    x = convBlock(x,filters*8)\n",
    "    x = convBlock(x,filters*8)\n",
    "    x = convBlock(x,filters*8)\n",
    "    x = MaxPooling2D(pool_size=(2,2),strides=2)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Dense(4096,kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Dense(4096,kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Dense(n_Class)(x)\n",
    "    x = Softmax(axis=-1)(x)\n",
    "    \n",
    "    return Model(inputs=input_tensor,outputs=x)\n",
    "\n",
    "def preprocessImageVgg(image):\n",
    "    img = tf.image.random_crop(image,size=[224,224,3])\n",
    "    img /= 127.5\n",
    "    img -= 1.\n",
    "    \n",
    "    return img\n",
    "    \n",
    "def getDatasetFromImageFolder(imageFolder,batchSize=32,n_epochs=100):\n",
    "   \n",
    "    def preprocessVgg(ds):\n",
    "        \n",
    "        ds = ds.map(lambda image ,label: (preprocessImageVgg(image) , tf.reshape(label ,(270,))) , num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        return ds\n",
    "    \n",
    "    def augmentDataset(ds):\n",
    "        ds = ds.map(lambda x ,y: (tf.image.rot90(x, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)) ,y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        ds = ds.map(lambda x ,y: (tf.image.random_flip_left_right(x),y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        ds = ds.map(lambda x ,y: (tf.image.random_flip_up_down(x),y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        return ds\n",
    "    \n",
    "\n",
    "    def process_img(img):\n",
    "        img = tf.image.decode_jpeg(img, channels=3) \n",
    "        #img = tf.image.convert_image_dtype(img, tf.float32) \n",
    "        return tf.image.resize(img, [224, 224])\n",
    "    \n",
    "    def combine_images_labels(file_path: tf.Tensor):\n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = process_img(img)\n",
    "        label = getLabelFromFilePathTF(file_path)\n",
    "        \n",
    "        labelEncoded = encodingLabelDict[label.numpy().decode('utf-8')]\n",
    "        \n",
    "        return img, labelEncoded\n",
    "        \n",
    "        \n",
    "    def prepareDataset(ds, training=False):\n",
    "        ds = ds.shuffle(buffer_size=10000)        \n",
    "        ds = ds.map(lambda x : tf.py_function(func=combine_images_labels,\n",
    "                        inp=[x], Tout=(tf.float32,tf.uint8)),\n",
    "                        num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        \n",
    "        ds = preprocessVgg(ds)\n",
    "        if(training):\n",
    "            ds = augmentDataset(ds)\n",
    "            \n",
    "        ds = ds.repeat()\n",
    "        ds = ds.batch(batchSize)\n",
    "        \n",
    "        ds = ds.apply(tf.data.experimental.copy_to_device('/gpu:0'))\n",
    "        ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        \n",
    "        return ds\n",
    "        \n",
    "    datasets = datasetImageFolder(imageFolder)\n",
    "    \n",
    "    \n",
    "    dsTrain, dsTest, dsValid = datasets['train'] ,datasets['test'],datasets['valid']\n",
    "    \n",
    "    #encodingLabelDict = generateBinaryEncoding(getAllLabels(imageFolder))\n",
    "    encodingLabelDict = generateOneHotEncodeDict(getAllLabels(imageFolder))\n",
    "    steps_per_epoch_train =  dsTrain.cardinality().numpy() // batchSize\n",
    "    steps_per_epoch_valid =  dsValid.cardinality().numpy() // batchSize\n",
    "    labelDict = generateBinaryEncoding(getAllLabels(imageFolder))  \n",
    "    \n",
    "    dsTrain = prepareDataset(dsTrain,training=True)\n",
    "    \n",
    "    dsTest = prepareDataset(dsTest)\n",
    "    dsValid = prepareDataset(dsValid)\n",
    "    \n",
    "    return dsTrain,dsTest,dsValid,steps_per_epoch_train,steps_per_epoch_valid\n",
    "\n",
    "def schedule(epoch,lr):\n",
    "        return lr / max(10 * int(epoch/20) ,1)\n",
    "    \n",
    "    \n",
    "def trainVgg(model,trainDataset,validDataset,epochs=100,verbose=1,steps_per_epoch=None,steps_per_epoch_valid=None):\n",
    "    my_callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(patience=10),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}.{val_loss:02f}.h5', monitor='val_loss',  verbose=0, save_best_only=True),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "        tf.keras.callbacks.LearningRateScheduler(schedule, verbose=0)\n",
    "    ]\n",
    "    \n",
    "    if steps_per_epoch is not None and steps_per_epoch_valid is not None :\n",
    "        hist = model.fit(x=trainDataset,validation_data=validDataset,steps_per_epoch=steps_per_epoch,epochs=epochs, callbacks=my_callbacks , verbose=verbose,validation_steps=steps_per_epoch_valid,initial_epoch=15)\n",
    "    else:\n",
    "        hist = model.fit(trainDataset,validation_data=validDataset,callbacks=my_callbacks,verbose=verbose)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ea8cc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270\n"
     ]
    }
   ],
   "source": [
    "n_labels = len( getAllLabels('data/'))\n",
    "n_Class = int(math.log(n_labels,2))+1\n",
    "print( n_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a6e5079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_vgg = vgg(filters=64,n_Class=n_labels,shape=(224,224,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d572e942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 270)\n"
     ]
    }
   ],
   "source": [
    "dsTrain,dsTest,dsValid ,stepTrain,steps_per_epoch_valid = getDatasetFromImageFolder('data/',batchSize=16)\n",
    "\n",
    "for image, label in dsTrain.take(1):\n",
    "    print(label.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "027cb0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2407\n",
      "84\n",
      "(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 270), dtype=tf.uint8, name=None))\n"
     ]
    }
   ],
   "source": [
    "print(stepTrain)\n",
    "print(steps_per_epoch_valid)\n",
    "print(dsTrain.element_spec)\n",
    "\n",
    "#checkSavedModel(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a2a161c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.15.3.005072.h5\n",
      "Epoch 16/100\n",
      "  98/2407 [>.............................] - ETA: 14:58 - loss: 6.6402 - categorical_accuracy: 0.0300"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ca93e6b208ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#model_vgg.load_weights(model)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainVgg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_vgg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainDataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdsTrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidDataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdsValid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstepTrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch_valid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-b8b1d00d99e4>\u001b[0m in \u001b[0;36mtrainVgg\u001b[1;34m(model, trainDataset, validDataset, epochs, verbose, steps_per_epoch, steps_per_epoch_valid)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msteps_per_epoch_valid\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainDataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidDataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmy_callbacks\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch_valid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainDataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidDataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmy_callbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\edoar\\documents\\github\\trainml\\training\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\edoar\\documents\\github\\trainml\\training\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\edoar\\documents\\github\\trainml\\training\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    860\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mc:\\users\\edoar\\documents\\github\\trainml\\training\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\edoar\\documents\\github\\trainml\\training\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\edoar\\documents\\github\\trainml\\training\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\edoar\\documents\\github\\trainml\\training\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def compileVgg(model ,learning_rate=1e-2):\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate,clipnorm=1,nesterov=True),loss='categorical_crossentropy',metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "    \n",
    "model = loadBestSavedModel(\".\")    \n",
    "\n",
    "model_vgg = tf.keras.models.load_model(model)\n",
    "#compileVgg(model_vgg,learning_rate=1e-2)\n",
    "#model_vgg.load_weights(model)\n",
    "\n",
    "hist = trainVgg(model_vgg,trainDataset=dsTrain,validDataset=dsValid,epochs=100,verbose=1,steps_per_epoch=stepTrain,steps_per_epoch_valid=steps_per_epoch_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7b60e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
